{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import commonprefix\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from Bio import SeqIO\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure parameters\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('paper')\n",
    "mpl.rcParams['axes.titlesize'] = 8\n",
    "mpl.rcParams['axes.labelsize'] = 8\n",
    "mpl.rcParams['xtick.labelsize'] = 6\n",
    "mpl.rcParams['ytick.labelsize'] = 6\n",
    "\n",
    "colours = {'DNA': '#fab95b', \n",
    "           'LTR': '#665c84', \n",
    "           'LINE': '#71a0a5', \n",
    "           'RC': '#ED7F35',\n",
    "           'SINE': '#5E96B5',\n",
    "           'Other': '#212121'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation and summary of results\n",
    "Since many salient features of the _Danio rerio_ transposable element landscape are calculated from different sources, in this notebook I have aggregated these and produced some summary figures.\n",
    "\n",
    "Age estimates are calculated by two methods, the first being percent sequence divergence from consensus, and the second being terminal branch lengths from phylogenetic trees. The former is taken from parsed RepeatMasker output files, and the latter from trees generated from reconstructed elements. See `../scripts/trees` folder for details.\n",
    "\n",
    "Copy number is calculated from the reconstructed elements, and element length is calculated directly from repbase consensus sequences. Gene distance is calculated (in `distribution.ipynb`) from distances between elements and genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes to ignore\n",
    "ignoreclasses = ['Simple_repeat', 'Satellite', 'Low_complexity', 'rRNA', 'tRNA', 'scRNA', 'snRNA', 'ARTEFACT']\n",
    "\n",
    "def clean_names(tename):\n",
    "    tename = tename.strip('-int')  # Remove pointless int tag for some LTRs\n",
    "    tename = tename.replace('_', '-')  # use only hyphens to match required scRNA format \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining LTRs and internal regions\n",
    "First we have some tricky steps to handle LTR elements. Since RepeatMasker separates internal and LTR regions for LTR-class TEs, we need to combine these to properly assess mean divergence and copynumber of individual LTR elements. Similarly, since full-length LTR elements comprise a pair of LTRs flanking the internal region, the length calculations need to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ltrdict():\n",
    "    ltrdict = {}\n",
    "    with open('../data/repeatmasker-out/defragmented/ltrdict.txt') as infile:\n",
    "        for line in infile:\n",
    "            line = line.split()\n",
    "            if len(line) == 2:  # i.e. both internal and LTR regions are present.\n",
    "                element = commonprefix(line) \n",
    "                for suffix in ['-', '_', '_DR', '_DRe']:\n",
    "                    element = element.strip(suffix)\n",
    "                # Some LTRs have multiple names; this loop handles these cases\n",
    "                for rid in line:\n",
    "                    if ':' in rid:\n",
    "                        rid = rid.split(':')\n",
    "                        for i in rid:\n",
    "                            ltrdict[i] = element\n",
    "                    else:\n",
    "                        ltrdict[rid] = element\n",
    "            elif len(line) == 1:  # i.e. only solo LTRs or Int fragments are present.\n",
    "                ltrdict[line[0]] = line[0]\n",
    "    return ltrdict\n",
    "\n",
    "def reverse_ltrdict():\n",
    "    ltrdict = get_ltrdict()\n",
    "    revdict = defaultdict(list)\n",
    "    for key, val in ltrdict.items():\n",
    "        revdict[val].append(key)\n",
    "    return revdict\n",
    "\n",
    "def handle_multi_ids(teid):\n",
    "    if ':' in teid:\n",
    "        teid = teid.split(':')\n",
    "        for i in teid:\n",
    "            if i in seqlength_dict.keys():\n",
    "                return i            \n",
    "    else:\n",
    "        return teid\n",
    "    \n",
    "def get_ltrlengths():    \n",
    "    ltr_lengths = {}\n",
    "    with open('../data/repeatmasker-out/defragmented/ltrdict.txt') as infile:\n",
    "        for line in infile:\n",
    "            line = line.split()\n",
    "            if len(line) == 2:\n",
    "                internal, ltr = handle_multi_ids(line[0]), handle_multi_ids(line[1])\n",
    "                ltr_lengths[internal] = seqlength_dict.get(internal, np.nan) + 2*seqlength_dict.get(ltr, np.nan)\n",
    "                ltr_lengths[ltr] = 0\n",
    "    return ltr_lengths\n",
    "    \n",
    "def map_ltr_lengths(x):\n",
    "    ltr_lengths = get_ltrlengths()\n",
    "    if x in ltr_lengths.keys():\n",
    "        return ltr_lengths[x]\n",
    "    else:\n",
    "        return seqlength_dict.get(x, np.nan)\n",
    "    \n",
    "ltrlist = []\n",
    "with open('../data/repeatmasker-out/defragmented/ltrdict.txt') as infile:\n",
    "    for line in infile:\n",
    "        line = line.split()\n",
    "        if len(line) == 2:\n",
    "            ltrlist.append(line[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsed RepeatMasker files from Aurelie Kapusta's `ParseRM.pl` scripts\n",
    "This gives us information such as genome coverage and mean percent-divergence from consensus sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tename     tefam teclass    meandiv   meandel   meanins       cov  \\\n",
      "0  Gypsy69-LTR_DR     Gypsy     LTR  11.775656  3.173951  2.493011  0.000017   \n",
      "1  Gypsy88-LTR_DR     Gypsy     LTR  15.967192  1.834374  1.008982  0.000014   \n",
      "2    Gypsy12-I_DR     Gypsy     LTR   5.864317  1.178092  0.543313  0.000020   \n",
      "3         KibiDr2    L1-Tx1    LINE   7.772816  3.898475  1.978873  0.000032   \n",
      "4     DNA-5-8B_DR  hAT-hAT5     DNA  13.801350  8.724557  5.089908  0.000319   \n",
      "\n",
      "   frags  \n",
      "0    113  \n",
      "1    144  \n",
      "2     52  \n",
      "3     93  \n",
      "4   2372  \n",
      "(2286, 8)\n",
      "1931\n"
     ]
    }
   ],
   "source": [
    "# Parsed RepeatMasker files from Aurelie Kapusta's `ParseRM.pl` scripts\n",
    "parserm_df = pd.read_csv('../data/repeatmasker-out/parserm/danRer11.nonalt.fa.align.parseRM.all-repeats.tab', \n",
    "                    header=None,\n",
    "                    skiprows=1,\n",
    "                    names=['tename', 'teclass', 'tefam', '1', '2', 'frags', '4', \n",
    "                           'meandiv', '5', 'meandel', '6', 'meanins', '7', '8', 'cov'],\n",
    "                    sep='\\t')[['tename', 'tefam', 'teclass', 'meandiv', 'meandel', 'meanins', 'cov', 'frags']]\n",
    "\n",
    "parserm_df = parserm_df.loc[~parserm_df['teclass'].isin(ignoreclasses)]\n",
    "\n",
    "ltrdict = get_ltrdict()\n",
    "\n",
    "print(parserm_df.head())\n",
    "print(parserm_df.shape)\n",
    "print(len(set(parserm_df['tename'].apply(lambda x: ltrdict.get(x, x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of copy number\n",
    "In order to get a more accurate assessment of copynumber that RepeatMaskers fragment count, we used `one_code_to_find_them_all.pl` to recover fragmented and nested repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonwells/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,2,3,5,6,7,12,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tename  copynum\n",
      "0       ACROBAT1     3277\n",
      "1       ACROBAT2      201\n",
      "2          ANGEL    60993\n",
      "3    Academ-1_DR       36\n",
      "4  Academ-N1_DRe     2324\n",
      "(1925, 2)\n"
     ]
    }
   ],
   "source": [
    "# Defragmented elements generated by `one_code_to_find_them_all.pl`\n",
    "\n",
    "copynum_df = pd.read_csv('../data/repeatmasker-out/defragmented/danRer11.nonalt.fa.out.elem_sorted.csv', \n",
    "                        sep='\\t')\n",
    "copynum_df = copynum_df.loc[copynum_df['Score'].str.startswith('###')] \\\n",
    "    .groupby('Element') \\\n",
    "    .agg({'Element': ['count']}) \\\n",
    "    .reset_index()\n",
    "copynum_df.columns = ['tename', 'copynum']\n",
    "\n",
    "copynum_df['tename'] = copynum_df['tename'].apply(lambda x: x.strip('-int'))\n",
    "\n",
    "ltrdict = get_ltrdict()\n",
    "\n",
    "mergedcopy_df = copynum_df.copy()\n",
    "mergedcopy_df['tename'] = mergedcopy_df['tename'].apply(lambda x: ltrdict.get(x, x))\n",
    "mergedcopy_df = mergedcopy_df.groupby('tename').sum().reset_index()\n",
    "mergedcopy_df.columns = ['tename', 'copynum']\n",
    "\n",
    "print(mergedcopy_df.head())\n",
    "print(mergedcopy_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of median distances of elements from nearby genes.\n",
    "This includes the median distance of insertions from a gene on either strand, and also separately for genes on the same or opposite strands. These distances are calculated using bed files generated from the defragmented insertions found by `one code to find them all`. For further details of the calculation, see `featurecoverage.sh` and `scripts/shuffle_tebed.{sh,py}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tename  dist   ssdist   dsdist\n",
      "0       ACROBAT1   0.0  19259.0  20637.0\n",
      "1       ACROBAT2   0.0   5653.5  12110.0\n",
      "2          ANGEL   0.0  12528.5  11574.0\n",
      "3    Academ-1_DR   0.0  29085.0  24506.0\n",
      "4  Academ-N1_DRe   0.0   4564.0   6563.0\n",
      "(1807, 4)\n"
     ]
    }
   ],
   "source": [
    "# Calculate median distance of elements from nearby genes.\n",
    "\n",
    "def parse_bed(filename):\n",
    "    \"\"\"This function parses bed files returned from `bedtools closest`\"\"\"\n",
    "    chroms = [f'chr{i}' for i in range(1,26)]\n",
    "    \n",
    "    bed_df = pd.read_csv(filename, \n",
    "                         sep='\\t',\n",
    "                         header=None,\n",
    "                         names=['chrom', 'testart', 'teend', 'tename', 'tefam', 'strand',\n",
    "                                'chrom2', 'genestart', 'geneend', 'genename', 'dist'],\n",
    "                         index_col=False)\n",
    "    bed_df = bed_df[['chrom', 'tename', 'tefam', 'dist']]\n",
    "    bed_df = bed_df.loc[bed_df['chrom'].isin(chroms)]\n",
    "    bed_df['teclass'] = bed_df['tefam'].apply(lambda x: x.split('/')[0])\n",
    "    bed_df['tefam'] = bed_df['tefam'].apply(lambda x: x.split('/')[-1])\n",
    "    \n",
    "    # Calculate median distance for each tename\n",
    "    bed_df = bed_df \\\n",
    "        .groupby(['tename']) \\\n",
    "        .median() \\\n",
    "        .reset_index()\n",
    "    \n",
    "    return bed_df\n",
    "\n",
    "meddist_df = parse_bed('../data/dist/GRCz11_defrag_te_closest_gene.bed')\n",
    "ss_df = parse_bed('../data/dist/GRCz11_defrag_te_closest_gene_ss.bed').rename({'dist': 'ssdist'}, axis=1)\n",
    "ds_df = parse_bed('../data/dist/GRCz11_defrag_te_closest_gene_ds.bed').rename({'dist': 'dsdist'}, axis=1)\n",
    "meddist_df = meddist_df.merge(ss_df, on='tename').merge(ds_df, on='tename')\n",
    "print(meddist_df.head())\n",
    "print(meddist_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE length\n",
    "Very simple calculation of consensus sequence lengths from RepeatMasker/Repbase libraries. For LTRs, this is calculated as 2*L + I, where L and I are the length of the LTR consensus and Internal consensus, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tename  length\n",
      "0   hAT-N38_DR  1029.0\n",
      "1   TE-X-17_DR   452.0\n",
      "2    Tx1-49_DR  1583.0\n",
      "3  DNA-8-30_DR  1022.0\n",
      "4   hAT-N63_DR  1710.0\n",
      "(1975, 2)\n"
     ]
    }
   ],
   "source": [
    "# TE lengths from RepeatMasker/Repbase libraries\n",
    "seqlength_dict = SeqIO.to_dict(SeqIO.parse('../data/misc/zebrep.fa', 'fasta'))\n",
    "seqlength_dict = {key: len(val.seq) for key, val in seqlength_dict.items()}\n",
    "length_df = pd.DataFrame \\\n",
    "    .from_dict(seqlength_dict, \n",
    "               orient='index', \n",
    "               columns=['length']) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'tename'})\n",
    "\n",
    "length_df['length'] = length_df['tename'].apply(map_ltr_lengths)\n",
    "ltrdict = get_ltrdict()\n",
    "length_df['tename'] = length_df['tename'].apply(lambda x: ltrdict.get(x, x)) \n",
    "length_df = length_df.loc[length_df['length'] > 0]\n",
    "\n",
    "print(length_df.head())\n",
    "print(length_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE ages\n",
    "Since divergence from consensus is innacurate for TEs with significant subfamily structure, we generated trees of insertions for each family and recalculated age as the median terminal branch length of all leaves in the tree. For details see `scripts/trees/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tename   tefam teclass   meanlen    medlen        25        75  \\\n",
      "1618      L1-2_DR  L1-Tx1    LINE  0.006016  0.000000  0.000000  0.000660   \n",
      "511   ERV5_DR_LTR    ERV1     LTR  0.002734  0.000000  0.000000  0.000000   \n",
      "492      Gypsy106   Gypsy     LTR  0.036935  0.000550  0.000550  0.018420   \n",
      "323        BEL-45     Pao     LTR  0.021318  0.000620  0.000000  0.006205   \n",
      "1214        BEL19     Pao     LTR  0.016664  0.002315  0.000945  0.012925   \n",
      "\n",
      "           var  numbranches  \n",
      "1618  0.001919          910  \n",
      "511   0.000103           18  \n",
      "492   0.011029           29  \n",
      "323   0.007365          102  \n",
      "1214  0.000937           34  \n",
      "(1880, 9)\n"
     ]
    }
   ],
   "source": [
    "# Branch length data from pre-computed trees\n",
    "tree_df = pd.read_csv('../data/trees/trees_summary.txt', sep='\\t')\n",
    "print(tree_df.sort_values('medlen').head())\n",
    "print(tree_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell-type specificity index (tau)\n",
    "This one is a little more complicated. In order to quantify the degree of cell-type specificity for different TE families, we repurposed a previously described tissue specificity index, $\\tau$ (Kryuchkova-Mostacci & Robinson-Rechavi, 2017; Yanai et al., 2005). Rather than using the average expression in a given tissue, we calculated $\\tau$ using the average expression in each cluster (CPM/cells per cluster). We applied an additional filter requiring that TEs must be expressed in at least 5% of cells in at least one cluster in order to be labelled as \"robustly expressed\". This reduces the number of TE families with very high tau values arising as an artifact of low-level stochastic expression in small numbers of cells.\n",
    "\n",
    "Note that this is the only dataset which also contains gene information. Hence, we have split this into two separated dataframes for TEs and genes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tename  ZF3S_tau  ZFDOME_tau   ZFB_tau  ZF75_tau  ZF60_tau  ZFHIGH_tau  \\\n",
      "0  (AAATG)  0.864334    0.731162  0.946241  0.862353  0.796940         NaN   \n",
      "1  (AACTG)  0.937640    0.772971  0.921956  0.800273  0.849347    0.873723   \n",
      "2  (AAGTG)       NaN         NaN  0.991390       NaN  0.925700         NaN   \n",
      "3  (AATAG)  0.870893    0.777185  0.852168  0.616807  0.845445    0.596545   \n",
      "4  (AATTG)  0.946309    0.340420  0.903243  0.863054  0.821763         NaN   \n",
      "\n",
      "    ZFS_tau  ZF90_tau  ZF30_tau  ZF50_tau  ZFOBLONG_tau  ZF6S_tau   meantau  \n",
      "0       NaN  0.796188       NaN  0.814403           NaN  0.896568  0.838524  \n",
      "1  0.752088  0.932411  0.799158  0.904978           NaN  0.874761  0.856300  \n",
      "2       NaN       NaN       NaN       NaN           NaN       NaN  0.958545  \n",
      "3  0.711776  0.671990  0.729801  0.590693      0.610043  0.766288  0.719970  \n",
      "4  0.724970  0.941271  0.899421  0.877614           NaN  0.968718  0.828678  \n",
      "(2580, 14)\n"
     ]
    }
   ],
   "source": [
    "def calc_tau(x):\n",
    "    \"\"\"Calculate tau - a measure of tissue/cell specificity.\n",
    "    \n",
    "    Args:\n",
    "        x - array-like data\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    max_x = max(x)\n",
    "    tau = sum([1 - xi/max_x for xi in x])/(n - 1)\n",
    "    return tau\n",
    "\n",
    "# Calculation of tau separately for each developmental stage.\n",
    "dirname = '../data/expression/clustered-ae/'\n",
    "tau_dfs = []\n",
    "# This loop extracts measures of tau from each developmental stage and adds to list of dataframes\n",
    "for filename in os.listdir(dirname):\n",
    "    if not filename.endswith('_AE.txt'):\n",
    "        continue\n",
    "    df = pd.read_csv(f'{dirname}/{filename}', sep='\\t', index_col=0)\n",
    "    stage = filename.split('_')[0]\n",
    "    df.columns = [f'{stage}_{i}' for i in df.columns]\n",
    "    df[f'{stage}_tau'] = df.apply(calc_tau, axis=1)  # This here does the actual tau calculation\n",
    "    tau_dfs.append(df[[f'{stage}_tau']])\n",
    "    \n",
    "# Then merge the dataframes and calculate the mean value of tau for every TE family\n",
    "tau_df = reduce(lambda x, y: x.merge(y, left_index=True, right_index=True, how='outer'), tau_dfs)\n",
    "tau_df['meantau'] = tau_df.mean(axis=1, skipna=True)\n",
    "tau_df.reset_index(inplace=True)\n",
    "tau_df.rename(columns={'index': 'tename'}, inplace=True)\n",
    "tau_df['tename'] = tau_df['tename'].apply(lambda x: x.strip('-int'))\n",
    "\n",
    "# Add infor for filtering of robustly expressed genes and TEs\n",
    "robustgeneexpr = [i.strip().strip('\"') for i in open('../data/expression/TE_gene_list_10%_0220.txt').readlines()]\n",
    "robustgeneexpr = [i for i in robustgeneexpr if i.startswith('ENSDARG')]\n",
    "robustteexpr5 = [i.strip().strip('\"') for i in open('../data/expression/TE.list.morethan5.txt').readlines()] + robustgeneexpr\n",
    "\n",
    "# Separate TE and genes into separate dataframes, and add some gene types\n",
    "ribosomal = list(pd.read_csv('../data/misc/Danio_rerio_ribosomal.txt', sep='\\t')['Gene stable ID'])\n",
    "homeobox = pd.read_csv('../data/misc/Danio_rerio_TF.txt', sep='\\t')\n",
    "homeobox = list(homeobox.loc[homeobox['Family'] == 'Homeobox', 'Ensembl'])\n",
    "\n",
    "gene_tau_df = tau_df.loc[tau_df['tename'].str.startswith('ENSDARG')].copy()\n",
    "gene_tau_df['tefam'] = 'Gene'\n",
    "gene_tau_df.loc[gene_tau_df['tename'].isin(ribosomal), 'tefam'] = 'Ribosomal\\ngenes'\n",
    "gene_tau_df.loc[gene_tau_df['tename'].isin(homeobox), 'tefam'] = 'Homeobox\\ntranscription factors'\n",
    "gene_tau_df['teclass'] = gene_tau_df['tefam']\n",
    "gene_tau_df['robust'] = False\n",
    "gene_tau_df.loc[gene_tau_df['tename'].isin(robustgeneexpr), 'robust'] = True\n",
    "gene_tau_df['isltr'] = False\n",
    "\n",
    "gene_tau_df.to_csv('../data/expression/gene_meantau.txt', sep='\\t', index=False)\n",
    "\n",
    "tau_df = tau_df.loc[~tau_df['tename'].str.startswith('ENSDARG')]\n",
    "\n",
    "print(tau_df.head())\n",
    "print(tau_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense/antisense ratio\n",
    "This is the ratio of the total number of sense reads to antisense reads across all cells and stages. It provides an indiciation of the degree to which TE expression is driven by either read-through transcription or internal promoters. A very high sense/antisense ratio is unlikely to be achieved by read-through transcription from neighboring genes as it would require either extensive targeting, strong selection or both, for te insertion downstream of genes. In contrast, a ratio close to 1:1 is most easily explained by random insertion orientation relative to genes, and subsequent read-through transcription in both sense and antisense directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tename     tefam teclass    meandiv   meandel   meanins       cov  \\\n",
      "0    Academ-1_DR  Academ-1     DNA   3.701289  1.353107  0.795203  0.000010   \n",
      "1  Academ-N1_DRe  Academ-1     DNA  16.467405  4.131268  2.755778  0.000229   \n",
      "2       ACROBAT1       PIF     DNA   6.093923  7.604549  1.702335  0.000542   \n",
      "3       ACROBAT2       PIF     DNA   9.027897  8.973347  2.816820  0.000059   \n",
      "4          ANGEL   Kolobok     DNA  16.961762  9.139754  2.392364  0.006887   \n",
      "\n",
      "   frags  sense/anti  \n",
      "0     39    5.018405  \n",
      "1   2236    1.259450  \n",
      "2   2947    0.962443  \n",
      "3    282    1.293771  \n",
      "4  65085    0.883911  \n",
      "(2286, 9)\n"
     ]
    }
   ],
   "source": [
    "antisense_df = pd.read_csv('../data/expression/sense_anti.txt', sep='\\t')\n",
    "print(antisense_df.head())\n",
    "print(antisense_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining datasets\n",
    "Finally, we merge together all of the previous dataframes, enabling downstream analyses in separate files. Outer joins are used for all, enabling subsets of this dataset to be used for all downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tename    tefam  teclass    meandiv       cov   frags  \\\n",
      "175           L1-2_DR   L1-Tx1     LINE   7.173071  0.000335  6103.0   \n",
      "833       ERV5_DR_LTR     ERV1      LTR   4.491648  0.000006    20.0   \n",
      "872     Gypsy106-I_DR    Gypsy      LTR   1.335491  0.000034    44.0   \n",
      "871   Gypsy106-LTR_DR    Gypsy      LTR   3.645278  0.000023    45.0   \n",
      "425    BEL-45_DRe-LTR      Pao      LTR  12.166720  0.000022   189.0   \n",
      "...               ...      ...      ...        ...       ...     ...   \n",
      "2203       TE-X-25_DR  Unknown  Unknown  10.470897  0.000018   111.0   \n",
      "2225       TE-X-26_DR  Unknown  Unknown  16.419407  0.000070   637.0   \n",
      "2232      hAT-N196_DR     hAT?      DNA  17.778350  0.000003    30.0   \n",
      "2262   Gypsy74-LTR_DR    Gypsy      LTR   6.374631  0.000007    38.0   \n",
      "2263     Gypsy74-I_DR    Gypsy      LTR   3.779143  0.000014    34.0   \n",
      "\n",
      "       mergedname flatmergedname         flatname  copynum     dist   ssdist  \\\n",
      "175       L1-2_DR        L1-2-DR          L1-2-DR   3179.0  43307.5  59152.0   \n",
      "833   ERV5_DR_LTR    ERV5-DR-LTR      ERV5-DR-LTR     19.0      NaN      NaN   \n",
      "872      Gypsy106       Gypsy106    Gypsy106-I-DR     69.0      0.0  23403.0   \n",
      "871      Gypsy106       Gypsy106  Gypsy106-LTR-DR     69.0      0.0  23403.0   \n",
      "425        BEL-45         BEL-45   BEL-45-DRe-LTR    134.0      0.0  36988.0   \n",
      "...           ...            ...              ...      ...      ...      ...   \n",
      "2203   TE-X-25_DR     TE-X-25-DR       TE-X-25-DR      NaN      NaN      NaN   \n",
      "2225   TE-X-26_DR     TE-X-26-DR       TE-X-26-DR      NaN      NaN      NaN   \n",
      "2232  hAT-N196_DR    hAT-N196-DR      hAT-N196-DR     30.0      0.0      0.0   \n",
      "2262      Gypsy74        Gypsy74   Gypsy74-LTR-DR     65.0   1445.5  20409.5   \n",
      "2263      Gypsy74        Gypsy74     Gypsy74-I-DR     65.0   1445.5  20409.5   \n",
      "\n",
      "       dsdist  length   medlen  isltr  \n",
      "175   63957.5  5534.0  0.00000  False  \n",
      "833       NaN   435.0  0.00000  False  \n",
      "872   14198.0  9189.0  0.00055  False  \n",
      "871   14198.0  9189.0  0.00055   True  \n",
      "425       0.0  6682.0  0.00062   True  \n",
      "...       ...     ...      ...    ...  \n",
      "2203      NaN   826.0      NaN  False  \n",
      "2225      NaN   582.0      NaN  False  \n",
      "2232  44612.0   471.0      NaN  False  \n",
      "2262  34552.0  8562.0      NaN   True  \n",
      "2263  34552.0  8562.0      NaN  False  \n",
      "\n",
      "[2286 rows x 16 columns]\n",
      "Empty DataFrame\n",
      "Columns: [tename, tefam, teclass, meandiv, cov, frags, mergedname, flatmergedname, flatname, copynum, dist, ssdist, dsdist, length, medlen, isltr]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Create second column for merged name, then map to either unique or merged name\n",
    "ltrdict = get_ltrdict()\n",
    "\n",
    "classes = {}\n",
    "with open('../data/repeatmasker-out/danRer11_classes_wredundant.txt') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip().split('\\t')\n",
    "        classes[line[0]] = line[1:]\n",
    "\n",
    "def fix_names(df, tename_col):\n",
    "    df['mergedname'] = df[tename_col].apply(lambda x: ltrdict.get(x, x))\n",
    "    df['flatmergedname'] = df['mergedname'].str.replace('_', '-')\n",
    "    df['flatname'] = df[tename_col].str.replace('_', '-')\n",
    "    return df\n",
    "\n",
    "summary_df = fix_names(parserm_df[['tename', 'tefam', 'teclass', 'meandiv', 'cov', 'frags']].copy(), 'tename')\n",
    "\n",
    "summary_df = summary_df.merge(mergedcopy_df, left_on='mergedname', right_on='tename', how='outer')\n",
    "summary_df.loc[summary_df.tename_x.isna(), 'tename_x'] = summary_df.loc[summary_df.tename_x.isna(), 'tename_y']\n",
    "summary_df = fix_names(summary_df, 'tename_x').drop('tename_y', axis=1).rename({'tename_x': 'tename'}, axis=1)\n",
    "\n",
    "summary_df = summary_df.merge(meddist_df, left_on='mergedname', right_on='tename', how='outer')\n",
    "summary_df.loc[summary_df.tename_x.isna(), 'tename_x'] = summary_df.loc[summary_df.tename_x.isna(), 'tename_y']\n",
    "summary_df = fix_names(summary_df, 'tename_x').drop('tename_y', axis=1).rename({'tename_x': 'tename'}, axis=1)\n",
    "\n",
    "summary_df = summary_df.merge(length_df, left_on='mergedname', right_on='tename', how='outer')\n",
    "summary_df.loc[summary_df.tename_x.isna(), 'tename_x'] = summary_df.loc[summary_df.tename_x.isna(), 'tename_y']\n",
    "summary_df = fix_names(summary_df, 'tename_x').drop('tename_y', axis=1).rename({'tename_x': 'tename'}, axis=1)\n",
    "\n",
    "summary_df = summary_df.merge(tree_df[['tename', 'medlen']], left_on='mergedname', right_on='tename', how='outer')\n",
    "summary_df.loc[summary_df.tename_x.isna(), 'tename_x'] = summary_df.loc[summary_df.tename_x.isna(), 'tename_y']\n",
    "summary_df = fix_names(summary_df, 'tename_x').drop('tename_y', axis=1).rename({'tename_x': 'tename'}, axis=1)\n",
    "\n",
    "summary_df.loc[summary_df.teclass.isna(), 'tefam'] = summary_df.loc[summary_df.teclass.isna()].apply(lambda x: classes['tename'][0])\n",
    "summary_df = summary_df.loc[~summary_df.tefam.isna()]\n",
    "\n",
    "summary_df.loc[summary_df['tefam'] == 'RC', 'teclass'] = 'RC'\n",
    "\n",
    "# # Mark LTR regions\n",
    "summary_df['isltr'] = False\n",
    "summary_df.loc[summary_df.tename.isin(ltrlist), 'isltr'] = True\n",
    "\n",
    "# Just in case\n",
    "summary_df = summary_df.drop_duplicates().sort_values('medlen')\n",
    "\n",
    "print(summary_df)\n",
    "print(summary_df.loc[summary_df.tename.isna()])\n",
    "\n",
    "summary_df.to_csv('../data/danrer11_te_summary.txt', sep='\\t', index=False, na_rep='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
